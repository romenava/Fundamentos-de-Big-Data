{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c305b702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://L2108017.bosonit.local:4041\n",
       "SparkContext available as 'sc' (version = 3.0.3, master = local[*], app id = local-1634294462128)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "cubed: Long => Long = $Lambda$1892/0x0000000801416420@162e6b2f\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cubed = (s:Long)=>{\n",
    "    s*s*s\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "365f14b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$1892/0x0000000801416420@162e6b2f,LongType,List(Some(class[value[0]: bigint])),Some(cubed),false,true)\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"cubed\",cubed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40697658",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(1,9).createOrReplaceTempView(\"udf_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72024838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|id_cubed|\n",
      "+---+--------+\n",
      "|  1|       1|\n",
      "|  2|       8|\n",
      "|  3|      27|\n",
      "|  4|      64|\n",
      "|  5|     125|\n",
      "|  6|     216|\n",
      "|  7|     343|\n",
      "|  8|     512|\n",
      "+---+--------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT id, cubed(id) AS id_cubed FROM udf_test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdea2ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t1: Array[Int] = Array(35, 36, 32, 30, 40, 42, 38)\r\n",
       "t2: Array[Int] = Array(31, 32, 34, 55, 56)\r\n",
       "tC: org.apache.spark.sql.DataFrame = [celsius: array<int>]\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t1 = Array(35, 36, 32, 30, 40, 42, 38)\n",
    "val t2 = Array(31, 32, 34, 55, 56)\n",
    "val tC = Seq(t1, t2).toDF(\"celsius\")\n",
    "tC.createOrReplaceTempView(\"tC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b661a082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|celsius                     |\n",
      "+----------------------------+\n",
      "|[35, 36, 32, 30, 40, 42, 38]|\n",
      "|[31, 32, 34, 55, 56]        |\n",
      "+----------------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "tC.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af775a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------------------+\n",
      "|celsius                     |fahrenheit_temp                |\n",
      "+----------------------------+-------------------------------+\n",
      "|[35, 36, 32, 30, 40, 42, 38]|[95, 96, 89, 86, 104, 107, 100]|\n",
      "|[31, 32, 34, 55, 56]        |[87, 89, 93, 131, 132]         |\n",
      "+----------------------------+-------------------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "// La función reduce no existe en Jupyter Notebook\n",
    "\n",
    "/*\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius, \n",
    " reduce(\n",
    " celsius, \n",
    " 0, \n",
    " (t, acc) -> t + acc, \n",
    " acc -> (acc div size(celsius) * 9 div 5) + 32\n",
    " ) as avgFahrenheit \n",
    " FROM tC\n",
    "\"\"\").show(false)\n",
    "*/\n",
    "\n",
    "//Este sería el equivalente usando transform\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius,\n",
    "       transform (\n",
    "           celsius,\n",
    "           t -> ((t * 9) div 5) + 32\n",
    "           ) as fahrenheit_temp       \n",
    "FROM tC\n",
    "\"\"\").show(10,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38423ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n",
       "delaysPath: String = C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\r\n",
       "airportsPath: String = C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/airport-codes-na.txt\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "val delaysPath = \"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "val airportsPath = \"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/airport-codes-na.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48369b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airports: org.apache.spark.sql.DataFrame = [City: string, State: string ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val airports = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"inferSchema\",\"true\")\n",
    ".option(\"delimiter\", \"\\t\")\n",
    ".csv(airportsPath)\n",
    "\n",
    "airports.createOrReplaceTempView(\"airports_na\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59b6714c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "delays: org.apache.spark.sql.DataFrame = [date: string, delay: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val delays = spark.read\n",
    ".option(\"header\",\"true\")\n",
    ".csv(delaysPath)\n",
    ".withColumn(\"delay\", expr(\"CAST(delay as INT) as delay\"))\n",
    ".withColumn(\"distance\", expr(\"CAST(distance as INT) as distance\"))\n",
    "\n",
    "delays.createOrReplaceTempView(\"departureDelays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ffea150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foo: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [date: string, delay: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val foo = delays.filter(expr(\"\"\"origin == 'SEA' AND destination == 'SFO' AND date like '01010%' AND delay > 0\"\"\"))\n",
    "foo.createOrReplaceTempView(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2cd763c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Abbotsford|   BC| Canada| YXX|\n",
      "|   Aberdeen|   SD|    USA| ABR|\n",
      "|    Abilene|   TX|    USA| ABI|\n",
      "|      Akron|   OH|    USA| CAK|\n",
      "|    Alamosa|   CO|    USA| ALS|\n",
      "|     Albany|   GA|    USA| ABY|\n",
      "|     Albany|   NY|    USA| ALB|\n",
      "|Albuquerque|   NM|    USA| ABQ|\n",
      "| Alexandria|   LA|    USA| AEX|\n",
      "|  Allentown|   PA|    USA| ABE|\n",
      "+-----------+-----+-------+----+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM airports_na LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "939d63fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM departureDelays LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56c0b6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM foo\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af727f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bar: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [date: string, delay: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bar = delays.union(foo)\n",
    "bar.createOrReplaceTempView(\"bar\")\n",
    "bar.filter(expr(\"\"\"origin == 'SEA' AND destination == 'SFO' AND date LIKE '01010%' AND delay > 0\"\"\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b864d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * \n",
    " FROM bar \n",
    " WHERE origin = 'SEA' \n",
    " AND destination = 'SFO' \n",
    " AND date LIKE '01010%' \n",
    " AND delay > 0\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b0a39af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "foo.join(airports.as(\"air\"), $\"air.IATA\" === $\"origin\").select(\"City\", \"State\", \"date\", \"delay\", \"distance\", \"destination\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66b810a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT a.City, a.State, f.date, f.delay, f.distance, f.destination \n",
    " FROM foo f\n",
    " JOIN airports_na a\n",
    " ON a.IATA = f.origin\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d70201ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac5dfe5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "import org.apache.spark.sql.types._\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@339a6673\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "val spark = SparkSession.builder.appName(\"SparkSQLExampleApp\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b86330b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS departureDelaysWindow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e8058c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: org.apache.spark.sql.DataFrame = [origin: string, destination: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE departureDelaysWindow AS\n",
    "SELECT origin, destination, SUM(delay) AS TotalDelays\n",
    " FROM departureDelays\n",
    "WHERE origin IN ('SEA', 'SFO', 'JFK')\n",
    " AND destination IN ('SEA', 'SFO', 'JFK', 'DEN', 'ORD', 'LAX', 'ATL')\n",
    "GROUP BY origin, destination\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6a37428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----------+\n",
      "|origin|destination|TotalDelays|\n",
      "+------+-----------+-----------+\n",
      "|   JFK|        ORD|       5608|\n",
      "|   SEA|        LAX|       9359|\n",
      "|   JFK|        SFO|      35619|\n",
      "|   SFO|        ORD|      27412|\n",
      "|   JFK|        DEN|       4315|\n",
      "|   SFO|        DEN|      18688|\n",
      "|   SFO|        SEA|      17080|\n",
      "|   SEA|        SFO|      22293|\n",
      "|   JFK|        ATL|      12141|\n",
      "|   SFO|        ATL|       5091|\n",
      "|   SEA|        DEN|      13645|\n",
      "|   SEA|        ATL|       4535|\n",
      "|   SEA|        ORD|      10041|\n",
      "|   JFK|        SEA|       7856|\n",
      "|   JFK|        LAX|      35755|\n",
      "|   SFO|        JFK|      24100|\n",
      "|   SFO|        LAX|      40798|\n",
      "|   SEA|        JFK|       4667|\n",
      "+------+-----------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM departureDelaysWindow\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32a38664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----------+\n",
      "|origin|destination|TotalDelays|\n",
      "+------+-----------+-----------+\n",
      "|   SEA|        SFO|      22293|\n",
      "|   SEA|        DEN|      13645|\n",
      "|   SEA|        ORD|      10041|\n",
      "+------+-----------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT origin, destination, sum(TotalDelays) as TotalDelays\n",
    "             FROM departureDelaysWindow\n",
    "             WHERE origin = 'SEA'\n",
    "             GROUP BY origin, destination\n",
    "             ORDER BY sum(TotalDelays) desc\n",
    "             LIMIT 3\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6590034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----------+----+\n",
      "|origin|destination|TotalDelays|rank|\n",
      "+------+-----------+-----------+----+\n",
      "|   SEA|        SFO|      22293|   1|\n",
      "|   SEA|        DEN|      13645|   2|\n",
      "|   SEA|        ORD|      10041|   3|\n",
      "|   SFO|        LAX|      40798|   1|\n",
      "|   SFO|        ORD|      27412|   2|\n",
      "|   SFO|        JFK|      24100|   3|\n",
      "|   JFK|        LAX|      35755|   1|\n",
      "|   JFK|        SFO|      35619|   2|\n",
      "|   JFK|        ATL|      12141|   3|\n",
      "+------+-----------+-----------+----+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT origin, destination, TotalDelays, rank\n",
    "             FROM (\n",
    "                 SELECT origin, destination, TotalDelays, dense_rank()\n",
    "                         OVER(PARTITION BY origin ORDER BY TotalDelays DESC) as rank\n",
    "                         FROM departureDelaysWindow)\n",
    "             WHERE rank <= 3\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c2f5324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "foo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40513fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.expr\r\n",
       "foo2: org.apache.spark.sql.DataFrame = [date: string, delay: int ... 4 more fields]\r\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.expr\n",
    "val foo2 = foo.withColumn(\"status\", expr(\"CASE WHEN delay <= 10 THEN 'On-time' ELSE 'Delayed' END\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8aae0f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+-------+\n",
      "|    date|delay|distance|origin|destination| status|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "|01010710|   31|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|  104|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|    5|     590|   SEA|        SFO|On-time|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "foo2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e5ebf87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------+\n",
      "|    date|distance|origin|destination| status|\n",
      "+--------+--------+------+-----------+-------+\n",
      "|01010710|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|     590|   SEA|        SFO|On-time|\n",
      "+--------+--------+------+-----------+-------+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "foo3: org.apache.spark.sql.DataFrame = [date: string, distance: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val foo3 = foo2.drop(\"delay\")\n",
    "foo3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f437a92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------------+\n",
      "|    date|distance|origin|destination|flight_status|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "|01010710|     590|   SEA|        SFO|      Delayed|\n",
      "|01010955|     590|   SEA|        SFO|      Delayed|\n",
      "|01010730|     590|   SEA|        SFO|      On-time|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "foo4: org.apache.spark.sql.DataFrame = [date: string, distance: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val foo4 = foo3.withColumnRenamed(\"status\", \"flight_status\")\n",
    "foo4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8393ec6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----+\n",
      "|destination|month|delay|\n",
      "+-----------+-----+-----+\n",
      "|        ORD|    1|   92|\n",
      "|        JFK|    1|   -7|\n",
      "|        DFW|    1|   -5|\n",
      "|        MIA|    1|   -3|\n",
      "|        DFW|    1|   -3|\n",
      "|        DFW|    1|    1|\n",
      "|        ORD|    1|  -10|\n",
      "|        DFW|    1|   -6|\n",
      "|        DFW|    1|   -2|\n",
      "|        ORD|    1|   -3|\n",
      "|        ORD|    1|    0|\n",
      "|        DFW|    1|   23|\n",
      "|        DFW|    1|   36|\n",
      "|        ORD|    1|  298|\n",
      "|        JFK|    1|    4|\n",
      "|        DFW|    1|    0|\n",
      "|        MIA|    1|    2|\n",
      "|        DFW|    1|    0|\n",
      "|        DFW|    1|    0|\n",
      "|        ORD|    1|   83|\n",
      "+-----------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT destination, cast(substring(date, 0, 2) as int) as month, delay\n",
    "            FROM departureDelays\n",
    "            WHERE origin = 'SEA'\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dc3222b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+------------+\n",
      "|destination|Jan_AvgDelay|Jan_MaxDelay|Feb_AvgDelay|Feb_MaxDelay|\n",
      "+-----------+------------+------------+------------+------------+\n",
      "|        ABQ|       19.86|         316|       11.42|          69|\n",
      "|        ANC|        4.44|         149|        7.90|         141|\n",
      "|        ATL|       11.98|         397|        7.73|         145|\n",
      "|        AUS|        3.48|          50|       -0.21|          18|\n",
      "|        BOS|        7.84|         110|       14.58|         152|\n",
      "|        BUR|       -2.03|          56|       -1.89|          78|\n",
      "|        CLE|       16.00|          27|        null|        null|\n",
      "|        CLT|        2.53|          41|       12.96|         228|\n",
      "|        COS|        5.32|          82|       12.18|         203|\n",
      "|        CVG|       -0.50|           4|        null|        null|\n",
      "|        DCA|       -1.15|          50|        0.07|          34|\n",
      "|        DEN|       13.13|         425|       12.95|         625|\n",
      "|        DFW|        7.95|         247|       12.57|         356|\n",
      "|        DTW|        9.18|         107|        3.47|          77|\n",
      "|        EWR|        9.63|         236|        5.20|         212|\n",
      "|        FAI|        1.84|         160|        4.21|          60|\n",
      "|        FAT|        1.36|         119|        5.22|         232|\n",
      "|        FLL|        2.94|          54|        3.50|          40|\n",
      "|        GEG|        2.28|          63|        2.87|          60|\n",
      "|        HDN|       -0.44|          27|       -6.50|           0|\n",
      "+-----------+------------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "error: error while loading Decimal, class file 'C:\\Spark\\jars\\spark-catalyst_2.12-3.0.3.jar(org/apache/spark/sql/types/Decimal.class)' is broken\r\n",
       "(class java.lang.RuntimeException/error reading Scala signature of Decimal.class: assertion failed:\r\n",
       "  Decimal$DecimalIsFractional\r\n",
       "     while compiling: <console>\r\n",
       "        during phase: globalPhase=terminal, enteringPhase=jvm\r\n",
       "     library version: version 2.12.10\r\n",
       "    compiler version: version 2.12.10\r\n",
       "  reconstructed args: -classpath  -Yrepl-class-based -Yrepl-outdir C:\\Users\\ALVARO~1.ROM\\AppData\\Local\\Temp\\tmps14mg5xl\r\n",
       "\r\n",
       "  last tree to typer: TypeTree(class Byte)\r\n",
       "       tree position: line 6 of <console>\r\n",
       "            tree tpe: Byte\r\n",
       "              symbol: (final abstract) class Byte in package scala\r\n",
       "   symbol definition: final abstract class Byte extends  (a ClassSymbol)\r\n",
       "      symbol package: scala\r\n",
       "       symbol owners: class Byte\r\n",
       "           call site: constructor $eval in object $eval in package $line50\r\n",
       "\r\n",
       "== Source file context for tree position ==\r\n",
       "\r\n",
       "     3\r\n",
       "     4 object $eval {\r\n",
       "     5   lazy val $result = INSTANCE.res30\r\n",
       "     6   lazy val $print: _root_.java.lang.String =  {\r\n",
       "     7     INSTANCE.$iw\r\n",
       "     8\r\n",
       "     9 val sb = new _root_.scala.StringBuilder)\r\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM(\n",
    "                SELECT destination, cast(substring(date,0,2)as int) as month, delay\n",
    "                FROM departureDelays\n",
    "                Where origin = 'SEA')\n",
    "                PIVOT(\n",
    "                    cast(avg(delay) as decimal(4,2)) as AvgDelay, max(delay) as MaxDelay FOR month in (1 Jan, 2 Feb)\n",
    "                    )\n",
    "                ORDER BY destination\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb298f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "/*\n",
    "    Pros y contras de utilizar UDFs:\n",
    "    -Pros:\n",
    "        -Pueden realizar operaciones de forma más rápida que las funciones predefinidas.\n",
    "        -Se puden usar directamente en Spark SQL.\n",
    "        -Operan por sesión, por lo que no persistirán en el metastore.\n",
    "    \n",
    "    -Cons:\n",
    "        -Spark SQl no garantiza el orden de evaluación de subexpressiones, por lo que para hacer el chequeo de nulos\n",
    "         hay que hacer el chequeo de nulos dentro del UDF o usar un if o case when.\n",
    "        \n",
    "*/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
