{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f566c819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubed(s):\n",
    "    return s*s*s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "226efdcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.cubed(s)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "spark.udf.register(\"cubed\",cubed, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20827efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(1,9).createOrReplaceTempView(\"udf_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66109fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|cubed_id|\n",
      "+---+--------+\n",
      "|  1|       1|\n",
      "|  2|       8|\n",
      "|  3|      27|\n",
      "|  4|      64|\n",
      "|  5|     125|\n",
      "|  6|     216|\n",
      "|  7|     343|\n",
      "|  8|     512|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT id, cubed(id) AS cubed_id FROM udf_test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c663c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "def cubed(a: pd.Series)->pd.Series:\n",
    "    return a*a*a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f615af0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "PyArrow >= 0.15.1 must be installed; however, it was not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-d755d27debf6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcubed_udf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas_udf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcubed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturnType\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLongType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\pandas\\functions.py\u001b[0m in \u001b[0;36mpandas_udf\u001b[1;34m(f, returnType, functionType)\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[1;31m# Note: 'X' means it throws an exception during the conversion.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[0mrequire_minimum_pandas_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m     \u001b[0mrequire_minimum_pyarrow_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[1;31m# decorator @pandas_udf(returnType, functionType)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\pandas\\utils.py\u001b[0m in \u001b[0;36mrequire_minimum_pyarrow_version\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mhave_arrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhave_arrow\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         raise ImportError(\"PyArrow >= %s must be installed; however, \"\n\u001b[0m\u001b[0;32m     54\u001b[0m                           \"it was not found.\" % minimum_pyarrow_version)\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyarrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminimum_pyarrow_version\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: PyArrow >= 0.15.1 must be installed; however, it was not found."
     ]
    }
   ],
   "source": [
    "cubed_udf = pandas_udf(cubed, returnType=LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "574de1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema=StructType([StructField(\"celsius\",ArrayType(IntegerType()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f60db59",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list = [[35,36,32,30,40,42,38]],[[31,32,34,55,56]]\n",
    "t_c=spark.createDataFrame(t_list, schema)\n",
    "t_c.createOrReplaceTempView(\"tC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d0d7b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|celsius                     |\n",
      "+----------------------------+\n",
      "|[35, 36, 32, 30, 40, 42, 38]|\n",
      "|[31, 32, 34, 55, 56]        |\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_c.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93c76bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------------------+\n",
      "|celsius                     |fahrenheit                     |\n",
      "+----------------------------+-------------------------------+\n",
      "|[35, 36, 32, 30, 40, 42, 38]|[95, 96, 89, 86, 104, 107, 100]|\n",
      "|[31, 32, 34, 55, 56]        |[87, 89, 93, 131, 132]         |\n",
      "+----------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT celsius, transform(celsius, t-> ((t* 9) div 5) +32) as fahrenheit FROM tC\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51e93fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------+\n",
      "|celsius                     |high    |\n",
      "+----------------------------+--------+\n",
      "|[35, 36, 32, 30, 40, 42, 38]|[40, 42]|\n",
      "|[31, 32, 34, 55, 56]        |[55, 56]|\n",
      "+----------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT celsius, filter(celsius, t -> t > 38) as high FROM tC\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "507e0307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------+\n",
      "|celsius                     |threshold|\n",
      "+----------------------------+---------+\n",
      "|[35, 36, 32, 30, 40, 42, 38]|true     |\n",
      "|[31, 32, 34, 55, 56]        |false    |\n",
      "+----------------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT celsius, exists(celsius, t -> t = 38) as threshold FROM tC\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27894c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------------------+\n",
      "|celsius                     |fahrenheit_temp                |\n",
      "+----------------------------+-------------------------------+\n",
      "|[35, 36, 32, 30, 40, 42, 38]|[95, 96, 89, 86, 104, 107, 100]|\n",
      "|[31, 32, 34, 55, 56]        |[87, 89, 93, 131, 132]         |\n",
      "+----------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# La función reduce no exite en Jupyter Notebook\n",
    "\n",
    "#spark.sql(\"\"\"\n",
    "#SELECT celsius, \n",
    "# reduce(\n",
    "# celsius, \n",
    "# 0, \n",
    "# (t, acc) -> t + acc, \n",
    "# acc -> (acc div size(celsius) * 9 div 5) + 32\n",
    "# ) as avgFahrenheit \n",
    "# FROM tC\n",
    "#\"\"\").show()\n",
    "\n",
    "#Este sería el equivalente usando transform\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius,\n",
    "       transform (\n",
    "           celsius,\n",
    "           t -> ((t * 9) div 5) + 32\n",
    "           ) as fahrenheit_temp       \n",
    "FROM tC\n",
    "\"\"\").show(10,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dbe3ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "delaysPath = \"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    "airportsPath = \"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/airport-codes-na.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a61c27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = (spark.read\n",
    "           .option(\"header\",\"true\")\n",
    "           .option(\"inferSchema\",\"true\")\n",
    "           .option(\"delimiter\", \"\\t\")\n",
    "           .csv(airportsPath))\n",
    "\n",
    "airports.createOrReplaceTempView(\"airports_na\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de75ce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays = (spark.read\n",
    "           .option(\"header\",\"true\")\n",
    "           .csv(delaysPath)\n",
    "           .withColumn(\"delay\", expr(\"CAST(delay as INT) as delay\"))\n",
    "           .withColumn(\"distance\", expr(\"CAST(distance as INT) as distance\")))\n",
    "\n",
    "delays.createOrReplaceTempView(\"departureDelays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60ff6154",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = delays.filter(expr(\"\"\"origin == 'SEA' AND destination == 'SFO' AND date like '01010%' AND delay > 0\"\"\"))\n",
    "\n",
    "foo.createOrReplaceTempView(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1a238e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Abbotsford|   BC| Canada| YXX|\n",
      "|   Aberdeen|   SD|    USA| ABR|\n",
      "|    Abilene|   TX|    USA| ABI|\n",
      "|      Akron|   OH|    USA| CAK|\n",
      "|    Alamosa|   CO|    USA| ALS|\n",
      "|     Albany|   GA|    USA| ABY|\n",
      "|     Albany|   NY|    USA| ALB|\n",
      "|Albuquerque|   NM|    USA| ABQ|\n",
      "| Alexandria|   LA|    USA| AEX|\n",
      "|  Allentown|   PA|    USA| ABE|\n",
      "+-----------+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM airports_na LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f492c5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM departureDelays LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f0b06f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM foo\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d8b3d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bar = delays.union(foo)\n",
    "bar.createOrReplaceTempView(\"bar\")\n",
    "bar.filter(expr(\"\"\"origin == 'SEA' AND destination == 'SFO' AND date LIKE '01010%' AND delay > 0\"\"\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "388e16e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * \n",
    " FROM bar \n",
    " WHERE origin = 'SEA' \n",
    " AND destination = 'SFO' \n",
    " AND date LIKE '01010%' \n",
    " AND delay > 0\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c3d7f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo.join(airports, airports.IATA == foo.origin).select(\"City\", \"State\", \"date\", \"delay\", \"distance\", \"destination\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b212afbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT a.City, a.State, f.date, f.delay, f.distance, f.destination \n",
    " FROM foo f\n",
    " JOIN airports_na a\n",
    " ON a.IATA = f.origin\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88516083",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "058e8262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "spark = SparkSession.builder.appName(\"SparkSQLExampleApp\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e59c700",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-448bb5a933b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DROP TABLE IF EXISTS departureDelaysWindow\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    647\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \"\"\"\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS departureDelaysWindow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79ecf23f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-a905aaa83cde>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m spark.sql(\"\"\"CREATE TABLE departureDelaysWindow AS\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mSELECT\u001b[0m \u001b[0morigin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSUM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelay\u001b[0m\u001b[1;33m)\u001b[0m \u001b[0mAS\u001b[0m \u001b[0mTotalDelays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m  \u001b[0mFROM\u001b[0m \u001b[0mdepartureDelays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mWHERE\u001b[0m \u001b[0morigin\u001b[0m \u001b[0mIN\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'SEA'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SFO'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'JFK'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m  \u001b[0mAND\u001b[0m \u001b[0mdestination\u001b[0m \u001b[0mIN\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'SEA'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SFO'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'JFK'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'DEN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ORD'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'LAX'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ATL'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    647\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \"\"\"\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE departureDelaysWindow AS\n",
    "SELECT origin, destination, SUM(delay) AS TotalDelays\n",
    " FROM departureDelays\n",
    "WHERE origin IN ('SEA', 'SFO', 'JFK')\n",
    " AND destination IN ('SEA', 'SFO', 'JFK', 'DEN', 'ORD', 'LAX', 'ATL')\n",
    "GROUP BY origin, destination\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9119e22e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6612a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ea67436d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+-------+\n",
      "|    date|delay|distance|origin|destination| status|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "|01010710|   31|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|  104|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|    5|     590|   SEA|        SFO|On-time|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "foo2 = foo.withColumn(\"status\", expr(\"CASE WHEN delay <= 10 THEN 'On-time' ELSE 'Delayed' END\"))\n",
    "foo2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ab4eff3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------+\n",
      "|    date|distance|origin|destination| status|\n",
      "+--------+--------+------+-----------+-------+\n",
      "|01010710|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|     590|   SEA|        SFO|On-time|\n",
      "+--------+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo3 = foo2.drop(\"delay\")\n",
    "foo3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cf7d5e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------------+\n",
      "|    date|distance|origin|destination|flight_status|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "|01010710|     590|   SEA|        SFO|      Delayed|\n",
      "|01010955|     590|   SEA|        SFO|      Delayed|\n",
      "|01010730|     590|   SEA|        SFO|      On-time|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo4 = foo3.withColumnRenamed(\"status\", \"flight_status\")\n",
    "foo4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89ac3065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----+\n",
      "|destination|month|delay|\n",
      "+-----------+-----+-----+\n",
      "|        ORD|    1|   92|\n",
      "|        JFK|    1|   -7|\n",
      "|        DFW|    1|   -5|\n",
      "|        MIA|    1|   -3|\n",
      "|        DFW|    1|   -3|\n",
      "|        DFW|    1|    1|\n",
      "|        ORD|    1|  -10|\n",
      "|        DFW|    1|   -6|\n",
      "|        DFW|    1|   -2|\n",
      "|        ORD|    1|   -3|\n",
      "|        ORD|    1|    0|\n",
      "|        DFW|    1|   23|\n",
      "|        DFW|    1|   36|\n",
      "|        ORD|    1|  298|\n",
      "|        JFK|    1|    4|\n",
      "|        DFW|    1|    0|\n",
      "|        MIA|    1|    2|\n",
      "|        DFW|    1|    0|\n",
      "|        DFW|    1|    0|\n",
      "|        ORD|    1|   83|\n",
      "+-----------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT destination, cast(substring(date, 0, 2) as int) as month, delay\n",
    "            FROM departureDelays\n",
    "            WHERE origin = 'SEA'\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26406c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+------------+\n",
      "|destination|Jan_AvgDelay|Jan_MaxDelay|Feb_AvgDelay|Feb_MaxDelay|\n",
      "+-----------+------------+------------+------------+------------+\n",
      "|        ABQ|       19.86|         316|       11.42|          69|\n",
      "|        ANC|        4.44|         149|        7.90|         141|\n",
      "|        ATL|       11.98|         397|        7.73|         145|\n",
      "|        AUS|        3.48|          50|       -0.21|          18|\n",
      "|        BOS|        7.84|         110|       14.58|         152|\n",
      "|        BUR|       -2.03|          56|       -1.89|          78|\n",
      "|        CLE|       16.00|          27|        null|        null|\n",
      "|        CLT|        2.53|          41|       12.96|         228|\n",
      "|        COS|        5.32|          82|       12.18|         203|\n",
      "|        CVG|       -0.50|           4|        null|        null|\n",
      "|        DCA|       -1.15|          50|        0.07|          34|\n",
      "|        DEN|       13.13|         425|       12.95|         625|\n",
      "|        DFW|        7.95|         247|       12.57|         356|\n",
      "|        DTW|        9.18|         107|        3.47|          77|\n",
      "|        EWR|        9.63|         236|        5.20|         212|\n",
      "|        FAI|        1.84|         160|        4.21|          60|\n",
      "|        FAT|        1.36|         119|        5.22|         232|\n",
      "|        FLL|        2.94|          54|        3.50|          40|\n",
      "|        GEG|        2.28|          63|        2.87|          60|\n",
      "|        HDN|       -0.44|          27|       -6.50|           0|\n",
      "+-----------+------------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM(\n",
    "                SELECT destination, cast(substring(date,0,2)as int) as month, delay\n",
    "                FROM departureDelays\n",
    "                Where origin = 'SEA')\n",
    "                PIVOT(\n",
    "                    cast(avg(delay) as decimal(4,2)) as AvgDelay, max(delay) as MaxDelay FOR month in (1 Jan, 2 Feb)\n",
    "                    )\n",
    "                ORDER BY destination\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8108431",
   "metadata": {},
   "source": [
    "## EJERCICIO MYSQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d91e82e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-----------+------+----------+\n",
      "|emp_no|birth_date|first_name|  last_name|gender| hire_date|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "| 10001|1953-09-02|    Georgi|    Facello|     M|1986-06-26|\n",
      "| 10002|1964-06-02|   Bezalel|     Simmel|     F|1985-11-21|\n",
      "| 10003|1959-12-03|     Parto|    Bamford|     M|1986-08-28|\n",
      "| 10004|1954-05-01| Chirstian|    Koblick|     M|1986-12-01|\n",
      "| 10005|1955-01-21|   Kyoichi|   Maliniak|     M|1989-09-12|\n",
      "| 10006|1953-04-20|    Anneke|    Preusig|     F|1989-06-02|\n",
      "| 10007|1957-05-23|   Tzvetan|  Zielinski|     F|1989-02-10|\n",
      "| 10008|1958-02-19|    Saniya|   Kalloufi|     M|1994-09-15|\n",
      "| 10009|1952-04-19|    Sumant|       Peac|     F|1985-02-18|\n",
      "| 10010|1963-06-01| Duangkaew|   Piveteau|     F|1989-08-24|\n",
      "| 10011|1953-11-07|      Mary|      Sluis|     F|1990-01-22|\n",
      "| 10012|1960-10-04|  Patricio|  Bridgland|     M|1992-12-18|\n",
      "| 10013|1963-06-07| Eberhardt|     Terkki|     M|1985-10-20|\n",
      "| 10014|1956-02-12|     Berni|      Genin|     M|1987-03-11|\n",
      "| 10015|1959-08-19|  Guoxiang|  Nooteboom|     M|1987-07-02|\n",
      "| 10016|1961-05-02|  Kazuhito|Cappelletti|     M|1995-01-27|\n",
      "| 10017|1958-07-06| Cristinel|  Bouloucos|     F|1993-08-03|\n",
      "| 10018|1954-06-19|  Kazuhide|       Peha|     F|1987-04-03|\n",
      "| 10019|1953-01-23|   Lillian|    Haddadi|     M|1999-04-30|\n",
      "| 10020|1952-12-24|    Mayuko|    Warwick|     M|1991-01-26|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF = (spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"employees\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"alromeco\")\n",
    ".load())\n",
    "\n",
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dabfcc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|dept_no|         dept_name|\n",
      "+-------+------------------+\n",
      "|   d009|  Customer Service|\n",
      "|   d005|       Development|\n",
      "|   d002|           Finance|\n",
      "|   d003|   Human Resources|\n",
      "|   d001|         Marketing|\n",
      "|   d004|        Production|\n",
      "|   d006|Quality Management|\n",
      "|   d008|          Research|\n",
      "|   d007|             Sales|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departmentsDF = (spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"departments\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"alromeco\")\n",
    ".load())\n",
    "\n",
    "departmentsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87ab4ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+\n",
      "|emp_no|salary| from_date|   to_date|\n",
      "+------+------+----------+----------+\n",
      "| 10001| 60117|1986-06-26|1987-06-26|\n",
      "| 10001| 62102|1987-06-26|1988-06-25|\n",
      "| 10001| 66074|1988-06-25|1989-06-25|\n",
      "| 10001| 66596|1989-06-25|1990-06-25|\n",
      "| 10001| 66961|1990-06-25|1991-06-25|\n",
      "| 10001| 71046|1991-06-25|1992-06-24|\n",
      "| 10001| 74333|1992-06-24|1993-06-24|\n",
      "| 10001| 75286|1993-06-24|1994-06-24|\n",
      "| 10001| 75994|1994-06-24|1995-06-24|\n",
      "| 10001| 76884|1995-06-24|1996-06-23|\n",
      "| 10001| 80013|1996-06-23|1997-06-23|\n",
      "| 10001| 81025|1997-06-23|1998-06-23|\n",
      "| 10001| 81097|1998-06-23|1999-06-23|\n",
      "| 10001| 84917|1999-06-23|2000-06-22|\n",
      "| 10001| 85112|2000-06-22|2001-06-22|\n",
      "| 10001| 85097|2001-06-22|2002-06-22|\n",
      "| 10001| 88958|2002-06-22|9999-01-01|\n",
      "| 10002| 65828|1996-08-03|1997-08-03|\n",
      "| 10002| 65909|1997-08-03|1998-08-03|\n",
      "| 10002| 67534|1998-08-03|1999-08-03|\n",
      "+------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salariesDF = (spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"salaries\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"alromeco\")\n",
    ".load())\n",
    "\n",
    "salariesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e76fdc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+----------+----------+\n",
      "|emp_no|             title| from_date|   to_date|\n",
      "+------+------------------+----------+----------+\n",
      "| 10001|   Senior Engineer|1986-06-26|9999-01-01|\n",
      "| 10002|             Staff|1996-08-03|9999-01-01|\n",
      "| 10003|   Senior Engineer|1995-12-03|9999-01-01|\n",
      "| 10004|          Engineer|1986-12-01|1995-12-01|\n",
      "| 10004|   Senior Engineer|1995-12-01|9999-01-01|\n",
      "| 10005|      Senior Staff|1996-09-12|9999-01-01|\n",
      "| 10005|             Staff|1989-09-12|1996-09-12|\n",
      "| 10006|   Senior Engineer|1990-08-05|9999-01-01|\n",
      "| 10007|      Senior Staff|1996-02-11|9999-01-01|\n",
      "| 10007|             Staff|1989-02-10|1996-02-11|\n",
      "| 10008|Assistant Engineer|1998-03-11|2000-07-31|\n",
      "| 10009|Assistant Engineer|1985-02-18|1990-02-18|\n",
      "| 10009|          Engineer|1990-02-18|1995-02-18|\n",
      "| 10009|   Senior Engineer|1995-02-18|9999-01-01|\n",
      "| 10010|          Engineer|1996-11-24|9999-01-01|\n",
      "| 10011|             Staff|1990-01-22|1996-11-09|\n",
      "| 10012|          Engineer|1992-12-18|2000-12-18|\n",
      "| 10012|   Senior Engineer|2000-12-18|9999-01-01|\n",
      "| 10013|      Senior Staff|1985-10-20|9999-01-01|\n",
      "| 10014|          Engineer|1993-12-29|9999-01-01|\n",
      "+------+------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titlesDF = (spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"titles\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"alromeco\")\n",
    ".load())\n",
    "\n",
    "titlesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9eb3ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "empTitleSal = employeesDF.join(salariesDF,\n",
    "                employeesDF.emp_no == salariesDF.emp_no\n",
    "                ).join(titlesDF,\n",
    "                      employeesDF.emp_no == titlesDF.emp_no).select(employeesDF.emp_no, \"birth_date\", \"first_name\", \"last_name\", \"gender\", \"hire_date\", \"title\",\"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20acab2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+----------------+---------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|           title|avgSalary|\n",
      "+------+----------+----------+---------+------+----------+----------------+---------+\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|Technique Leader| 55591.73|\n",
      "| 10362|1963-09-16|   Shalesh|  dAstous|     M|1988-08-24|    Senior Staff|  47990.0|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07|        Engineer| 71811.64|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07| Senior Engineer| 71811.64|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26|    Senior Staff| 65324.67|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26|           Staff| 65324.67|\n",
      "| 11033|1957-03-01|   Shushma|     Bahk|     F|1990-10-02|        Engineer| 66597.75|\n",
      "| 11033|1957-03-01|   Shushma|     Bahk|     F|1990-10-02| Senior Engineer| 66597.75|\n",
      "| 11141|1957-08-20|   Vasiliy|Kermarrec|     F|1989-12-28|        Engineer|  49439.3|\n",
      "| 11141|1957-08-20|   Vasiliy|Kermarrec|     F|1989-12-28| Senior Engineer|  49439.3|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21|    Senior Staff| 47334.71|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21|           Staff| 47334.71|\n",
      "| 11458|1958-08-09|     Stevo|Chenoweth|     F|1985-10-06|    Senior Staff| 63011.29|\n",
      "| 11458|1958-08-09|     Stevo|Chenoweth|     F|1985-10-06|           Staff| 63011.29|\n",
      "| 11748|1953-03-07|    Lihong| Massonet|     M|1992-12-20|        Engineer| 63317.71|\n",
      "| 11858|1962-11-21|   Slavian|     Baik|     M|1988-11-12|           Staff| 43584.33|\n",
      "| 12027|1962-07-31|   Zhanqiu| Vuskovic|     M|1985-11-23|Technique Leader| 57875.71|\n",
      "| 12046|1961-05-16|     Xiong|    Ranum|     F|1996-01-13|           Staff| 73442.33|\n",
      "| 12799|1954-11-21|     Tiina| Businaro|     F|1993-12-25|    Senior Staff| 52852.67|\n",
      "| 12940|1953-10-25|  Odinaldo|   Farrar|     F|1987-12-12|        Engineer| 67110.47|\n",
      "+------+----------+----------+---------+------+----------+----------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "(empTitleSal\n",
    ".groupBy(employeesDF.emp_no, \"birth_date\", \"first_name\", \"last_name\", \"gender\", \"hire_date\", \"title\")\n",
    ".agg(round(avg(\"salary\"),2).alias(\"avgSalary\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e87ff3e",
   "metadata": {},
   "source": [
    "# DIFERENCIA ENTRE RANK Y DENSE_RANK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a988ec16",
   "metadata": {},
   "source": [
    "La diferencia entre rank y dense_rank es que si hay dos campos que tienen la misma posición, utilizando rank el siguiente campo se saltará un número, mientras que usando dense_rank no se saltará ningún número.\n",
    "Por ejemplo, si hay dos campos que tienen el rango 2, con rank el siguiente campo tendrá el número 4 mientras que con dense_rank tendrá el 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "750b2258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+\n",
      "|emp_no|dept_no| from_date|   to_date|\n",
      "+------+-------+----------+----------+\n",
      "| 10001|   d005|1986-06-26|9999-01-01|\n",
      "| 10002|   d007|1996-08-03|9999-01-01|\n",
      "| 10003|   d004|1995-12-03|9999-01-01|\n",
      "| 10004|   d004|1986-12-01|9999-01-01|\n",
      "| 10005|   d003|1989-09-12|9999-01-01|\n",
      "| 10006|   d005|1990-08-05|9999-01-01|\n",
      "| 10007|   d008|1989-02-10|9999-01-01|\n",
      "| 10008|   d005|1998-03-11|2000-07-31|\n",
      "| 10009|   d006|1985-02-18|9999-01-01|\n",
      "| 10010|   d006|2000-06-26|9999-01-01|\n",
      "| 10011|   d009|1990-01-22|1996-11-09|\n",
      "| 10012|   d005|1992-12-18|9999-01-01|\n",
      "| 10013|   d003|1985-10-20|9999-01-01|\n",
      "| 10014|   d005|1993-12-29|9999-01-01|\n",
      "| 10015|   d008|1992-09-19|1993-08-22|\n",
      "| 10016|   d007|1998-02-11|9999-01-01|\n",
      "| 10017|   d001|1993-08-03|9999-01-01|\n",
      "| 10018|   d004|1992-07-29|9999-01-01|\n",
      "| 10019|   d008|1999-04-30|9999-01-01|\n",
      "| 10020|   d004|1997-12-30|9999-01-01|\n",
      "+------+-------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "currentDept = (spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"current_dept_emp\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"alromeco\")\n",
    ".load())\n",
    "\n",
    "currentDept.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "53901485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+----------------+------+-------+-----------+----------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|           title|salary|dept_no|  dept_name| from_date|   to_date|\n",
      "+------+----------+----------+---------+------+----------+----------------+------+-------+-----------+----------+----------+\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|Technique Leader| 40000|   d005|Development|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|Technique Leader| 43519|   d005|Development|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|Technique Leader| 46265|   d005|Development|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|Technique Leader| 46865|   d005|Development|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|Technique Leader| 47837|   d005|Development|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|Technique Leader| 52042|   d005|Development|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|Technique Leader| 52370|   d005|Development|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|Technique Leader| 53202|   d005|Development|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|Technique Leader| 56087|   d005|Development|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|Technique Leader| 59252|   d005|Development|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|Technique Leader| 62716|   d005|Development|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|Technique Leader| 67137|   d005|Development|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|Technique Leader| 67944|   d005|Development|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|Technique Leader| 67588|   d005|Development|1988-04-19|9999-01-01|\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19|Technique Leader| 71052|   d005|Development|1988-04-19|9999-01-01|\n",
      "| 11033|1957-03-01|   Shushma|     Bahk|     F|1990-10-02|        Engineer| 57239|   d005|Development|1991-03-14|9999-01-01|\n",
      "| 11033|1957-03-01|   Shushma|     Bahk|     F|1990-10-02| Senior Engineer| 57239|   d005|Development|1991-03-14|9999-01-01|\n",
      "| 11033|1957-03-01|   Shushma|     Bahk|     F|1990-10-02|        Engineer| 60720|   d005|Development|1991-03-14|9999-01-01|\n",
      "| 11033|1957-03-01|   Shushma|     Bahk|     F|1990-10-02| Senior Engineer| 60720|   d005|Development|1991-03-14|9999-01-01|\n",
      "| 11033|1957-03-01|   Shushma|     Bahk|     F|1990-10-02|        Engineer| 61596|   d005|Development|1991-03-14|9999-01-01|\n",
      "+------+----------+----------+---------+------+----------+----------------+------+-------+-----------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empdDept=empTitleSal.join(currentDept,\n",
    "                 empTitleSal.emp_no == currentDept.emp_no\n",
    "                 ).join(departmentsDF,\n",
    "                       currentDept.dept_no == departmentsDF.dept_no\n",
    "                       ).select(empTitleSal.emp_no, \"birth_date\", \"first_name\", \"last_name\", \"gender\", \"hire_date\", \"title\",\"salary\",currentDept.dept_no,\"dept_name\",\"from_date\",\"to_date\")\n",
    "\n",
    "empdDept.persist()\n",
    "\n",
    "empdDept.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a62dc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "empdDept.select(\"emp_no\", \"first_name\", \"last_name\", \"gender\",\"dept_name\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
