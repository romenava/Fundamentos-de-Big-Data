{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bf06663",
   "metadata": {},
   "source": [
    "## Creando la sesiÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b9c1bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "import org.apache.spark.sql.types._\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@62142110\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "val spark = SparkSession.builder.appName(\"SparkSQLExampleApp\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a61f2d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "csvFile: String = C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val csvFile=\"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de8e522d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flightSchema: org.apache.spark.sql.types.StructType = StructType(StructField(date,StringType,true), StructField(delay,IntegerType,true), StructField(distance,IntegerType,true), StructField(origin,StringType,true), StructField(destination,StringType,true))\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flightSchema = StructType(Array(StructField(\"date\", StringType, true),\n",
    "                                   StructField(\"delay\", IntegerType, true),\n",
    "                                   StructField(\"distance\", IntegerType,true),\n",
    "                                   StructField(\"origin\", StringType, true),\n",
    "                                   StructField(\"destination\", StringType, true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f949fa48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [date: string, delay: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df =spark.read.format(\"csv\")\n",
    ".schema(flightSchema)\n",
    ".option(\"header\",\"true\")\n",
    ".load(csvFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3623089",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06a5e636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|date    |delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|6    |602     |ABE   |ATL        |\n",
      "|01020600|-8   |369     |ABE   |DTW        |\n",
      "|01021245|-2   |602     |ABE   |ATL        |\n",
      "|01020605|-4   |602     |ABE   |ATL        |\n",
      "|01031245|-4   |602     |ABE   |ATL        |\n",
      "|01030605|0    |602     |ABE   |ATL        |\n",
      "|01041243|10   |602     |ABE   |ATL        |\n",
      "|01040605|28   |602     |ABE   |ATL        |\n",
      "|01051245|88   |602     |ABE   |ATL        |\n",
      "|01050605|9    |602     |ABE   |ATL        |\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT *\n",
    "            FROM us_delay_flights_tbl\"\"\").show(10,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61f2d5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT distance,origin,destination \n",
    "            FROM us_delay_flights_tbl\n",
    "            WHERE distance >1000\n",
    "            ORDER BY distance DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f27df4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "df.select(\"distance\", \"origin\", \"destination\")\n",
    ".filter($\"distance\">1000)\n",
    ".orderBy(desc(\"distance\"))\n",
    ".show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "970782e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------+-----------+\n",
      "|delay|    date|origin|destination|\n",
      "+-----+--------+------+-----------+\n",
      "| 1638|02190925|   SFO|        ORD|\n",
      "|  396|01031755|   SFO|        ORD|\n",
      "|  326|01022330|   SFO|        ORD|\n",
      "|  320|01051205|   SFO|        ORD|\n",
      "|  297|01190925|   SFO|        ORD|\n",
      "|  296|02171115|   SFO|        ORD|\n",
      "|  279|01071040|   SFO|        ORD|\n",
      "|  274|01051550|   SFO|        ORD|\n",
      "|  266|03120730|   SFO|        ORD|\n",
      "|  258|01261104|   SFO|        ORD|\n",
      "+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT delay,date,origin,destination \n",
    "            FROM us_delay_flights_tbl\n",
    "            WHERE origin='SFO' and destination='ORD' and delay > 120 \n",
    "            ORDER BY delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "877c9193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------+-----------+\n",
      "|delay|    date|origin|destination|\n",
      "+-----+--------+------+-----------+\n",
      "| 1638|02190925|   SFO|        ORD|\n",
      "|  396|01031755|   SFO|        ORD|\n",
      "|  326|01022330|   SFO|        ORD|\n",
      "|  320|01051205|   SFO|        ORD|\n",
      "|  297|01190925|   SFO|        ORD|\n",
      "|  296|02171115|   SFO|        ORD|\n",
      "|  279|01071040|   SFO|        ORD|\n",
      "|  274|01051550|   SFO|        ORD|\n",
      "|  266|03120730|   SFO|        ORD|\n",
      "|  258|01261104|   SFO|        ORD|\n",
      "+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "df\n",
    ".select(\"delay\", \"date\", \"origin\", \"destination\")\n",
    ".filter($\"origin\"===\"SFO\" && $\"destination\"===\"ORD\" && $\"delay\">120)\n",
    ".orderBy(desc(\"delay\"))\n",
    ".show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cee374be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new_df: org.apache.spark.sql.DataFrame = [delay: int, distance: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val new_df=df.withColumn(\"DateTime\", to_timestamp(col(\"date\"), \"MMddHHmm\")).drop(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b17679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f59a1209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n",
      "|Day|Month|Count|\n",
      "+---+-----+-----+\n",
      "|  2|    1|    4|\n",
      "|  3|    1|    4|\n",
      "| 31|    3|    4|\n",
      "| 12|    3|    3|\n",
      "|  9|    2|    3|\n",
      "|  8|    2|    2|\n",
      "| 26|    3|    2|\n",
      "|  5|    1|    2|\n",
      "|  1|    1|    2|\n",
      "| 17|    3|    2|\n",
      "+---+-----+-----+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT day(DateTime) Day,month(DateTime) Month, count(*) Count\n",
    "            FROM us_delay_flights_tbl\n",
    "            WHERE origin='SFO' and destination='ORD' and delay > 120 \n",
    "            Group by month(DateTime), day(DateTime)\n",
    "            ORDER BY count(*) DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "209bc5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+-----+\n",
      "|month(DateTime)|dayofmonth(DateTime)|count|\n",
      "+---------------+--------------------+-----+\n",
      "|              1|                   2|    4|\n",
      "|              1|                   3|    4|\n",
      "|              3|                  31|    4|\n",
      "|              3|                  12|    3|\n",
      "|              2|                   9|    3|\n",
      "|              2|                  13|    2|\n",
      "|              1|                   5|    2|\n",
      "|              2|                  27|    2|\n",
      "|              1|                   1|    2|\n",
      "|              3|                  17|    2|\n",
      "+---------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "new_df\n",
    ".filter($\"origin\"===\"SFO\" && $\"destination\"===\"ORD\" && $\"delay\">120).groupBy(month($\"DateTime\"),dayofmonth($\"DateTime\"))\n",
    ".count()\n",
    ".orderBy(desc(\"count\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "aa81a498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    "             CASE\n",
    "                 WHEN delay >= 360 THEN 'Very Long Delays'\n",
    "                 WHEN delay >= 120 AND delay < 360 THEN 'Long Delays'\n",
    "                 WHEN delay >= 60 AND delay < 120 THEN 'Short Delays'\n",
    "                 WHEN delay >= 0 AND delay < 60 THEN 'Tolerable Delays'\n",
    "                 WHEN delay = 0 THEN 'No Delays'\n",
    "                 ELSE 'Early'\n",
    "             END AS Flight_Delays\n",
    "             FROM us_delay_flights_tbl\n",
    "             ORDER BY origin, delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a9c55aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "df\n",
    ".select($\"delay\", $\"origin\", $\"destination\",\n",
    "                 when(col(\"delay\") >= 360,\"Very Long Delays\")\n",
    "                 .when(col(\"delay\") >= 120 && col(\"delay\") < 360,\"Long Delays\")\n",
    "                 .when(col(\"delay\") >= 60 && col(\"delay\") < 120,\"Short Delays\")\n",
    "                 .when(col(\"delay\") >= 0 && col(\"delay\") < 60,\"Tolerable Delays\")\n",
    "                 .when(col(\"delay\") === 0, \"No Delays\")\n",
    "                 .otherwise(\"Early\")\n",
    "                 .alias(\"Flight_Delays\"))\n",
    ".orderBy(asc(\"origin\"),desc(\"delay\"))\n",
    ".show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f67e462",
   "metadata": {},
   "source": [
    "# TABLAS MANAGED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aceb76c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "import org.apache.spark.sql.types._\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@2c7d43eb\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "val spark = SparkSession.builder.appName(\"SparkSQLExampleApp\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60b37d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0404152",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Database learn_spark_db already exists;\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Database learn_spark_db already exists;\r",
      "  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:113)\r",
      "  at org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:194)\r",
      "  at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createDatabase(ExternalCatalogWithListener.scala:47)\r",
      "  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:215)\r",
      "  at org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:81)\r",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\r",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\r",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r",
      "  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)\r",
      "  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\r",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r",
      "  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:610)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\r",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:605)\r",
      "  ... 43 elided\r",
      "Caused by: org.apache.hadoop.hive.metastore.api.AlreadyExistsException: Database learn_spark_db already exists\r",
      "  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:925)\r",
      "  at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r",
      "  at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\r",
      "  at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r",
      "  at java.base/java.lang.reflect.Method.invoke(Method.java:564)\r",
      "  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)\r",
      "  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\r",
      "  at com.sun.proxy.$Proxy26.create_database(Unknown Source)\r",
      "  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:727)\r",
      "  at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r",
      "  at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\r",
      "  at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r",
      "  at java.base/java.lang.reflect.Method.invoke(Method.java:564)\r",
      "  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)\r",
      "  at com.sun.proxy.$Proxy27.createDatabase(Unknown Source)\r",
      "  at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:427)\r",
      "  at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createDatabase$1(HiveClientImpl.scala:340)\r",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r",
      "  at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:295)\r",
      "  at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:228)\r",
      "  at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:227)\r",
      "  at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:277)\r",
      "  at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:338)\r",
      "  at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createDatabase$1(HiveExternalCatalog.scala:194)\r",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r",
      "  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:103)\r",
      "  ... 65 more\r",
      ""
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca6762fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f06b3d21",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException",
     "evalue": " Table or view 'managed_us_delay_flights_tbl' already exists in database 'learn_spark_db';\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException: Table or view 'managed_us_delay_flights_tbl' already exists in database 'learn_spark_db';\r",
      "  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:321)\r",
      "  at org.apache.spark.sql.execution.command.CreateTableCommand.run(tables.scala:164)\r",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\r",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\r",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r",
      "  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)\r",
      "  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\r",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r",
      "  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:610)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\r",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:605)\r",
      "  ... 43 elided\r",
      ""
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE managed_us_delay_flights_tbl(date STRING, delay INT, distance INT, origin STRING, destination STRING)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a34a783b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-----------+\n",
      "|      database|           tableName|isTemporary|\n",
      "+--------------+--------------------+-----------+\n",
      "|learn_spark_db|funciona_us_delay...|      false|\n",
      "|learn_spark_db|managed_us_delay_...|      false|\n",
      "|learn_spark_db|prueba_us_delay_f...|      false|\n",
      "|learn_spark_db|us_delay_flights_tbl|      false|\n",
      "+--------------+--------------------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c126eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE us_delay_flights_tbl(date STRING, delay INT, distance INT, origin STRING, destination STRING)\n",
    "             USING csv OPTIONS (PATH 'C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv')\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f135bdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+--------------------+\n",
      "|          name|     description|         locationUri|\n",
      "+--------------+----------------+--------------------+\n",
      "|       default|default database|file:/C:/Users/al...|\n",
      "|learn_spark_db|                |file:/C:/Users/al...|\n",
      "+--------------+----------------+--------------------+\n",
      "\n",
      "+--------------------+--------------+-----------+---------+-----------+\n",
      "|                name|      database|description|tableType|isTemporary|\n",
      "+--------------------+--------------+-----------+---------+-----------+\n",
      "|us_delay_flights_tbl|learn_spark_db|       null| EXTERNAL|      false|\n",
      "+--------------------+--------------+-----------+---------+-----------+\n",
      "\n",
      "+-----------+-----------+--------+--------+-----------+--------+\n",
      "|       name|description|dataType|nullable|isPartition|isBucket|\n",
      "+-----------+-----------+--------+--------+-----------+--------+\n",
      "|       date|       null|  string|    true|      false|   false|\n",
      "|      delay|       null|     int|    true|      false|   false|\n",
      "|   distance|       null|     int|    true|      false|   false|\n",
      "|     origin|       null|  string|    true|      false|   false|\n",
      "|destination|       null|  string|    true|      false|   false|\n",
      "+-----------+-----------+--------+--------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.catalog.listDatabases().show()\n",
    "spark.catalog.listTables().show()\n",
    "spark.catalog.listColumns(\"us_delay_flights_tbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c434fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallNumber: integer (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: integer (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: integer (nullable = true)\n",
      " |-- ALSUnit: boolean (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumAlarms: integer (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: integer (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- ResponseDelayedMins: float (nullable = true)\n",
      " |-- IncidentDate: timestamp (nullable = true)\n",
      " |-- OnWatchDate: timestamp (nullable = true)\n",
      " |-- AvailableDtTS: timestamp (nullable = true)\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df=spark.read.format(\"parquet\").load(\"C:/Users/alvaro.romero/Big_Data/Ejercicios_Spark/Guardar_Tablas/Parquet/*\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af9a4582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ALSUnit: boolean (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- AvailableDtTS: string (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- CallNumber: long (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- FinalPriority: long (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- IncidentDate: string (nullable = true)\n",
      " |-- IncidentNumber: long (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- NumAlarms: long (nullable = true)\n",
      " |-- OnWatchDate: string (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- ResponseDelayedMins: double (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- SupervisorDistrict: string (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: long (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- Zipcode: long (nullable = true)\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [ALSUnit: boolean, Address: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2=spark.read.format(\"json\").load(\"C:/Users/alvaro.romero/Big_Data/Ejercicios_Spark/Guardar_Tablas/JSON/*\")\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f44229e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: integer (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: integer (nullable = true)\n",
      " |-- _c14: boolean (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: integer (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: integer (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      " |-- _c20: string (nullable = true)\n",
      " |-- _c21: string (nullable = true)\n",
      " |-- _c22: string (nullable = true)\n",
      " |-- _c23: string (nullable = true)\n",
      " |-- _c24: double (nullable = true)\n",
      " |-- _c25: timestamp (nullable = true)\n",
      " |-- _c26: timestamp (nullable = true)\n",
      " |-- _c27: timestamp (nullable = true)\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df3: org.apache.spark.sql.DataFrame = [_c0: int, _c1: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df3=spark.read.format(\"csv\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".option(\"header\",\"false\")\n",
    ".option(\"mode\",\"PERMISSIVE\")\n",
    ".option(\"separator\",\",\")\n",
    ".load(\"C:/Users/alvaro.romero/Big_Data/Ejercicios_Spark/Guardar_Tablas/CSV/*\")\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a98f739c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallNumber: integer (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: integer (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: integer (nullable = true)\n",
      " |-- ALSUnit: boolean (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumAlarms: integer (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: integer (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- ResponseDelayedMins: float (nullable = true)\n",
      " |-- IncidentDate: timestamp (nullable = true)\n",
      " |-- OnWatchDate: timestamp (nullable = true)\n",
      " |-- AvailableDtTS: timestamp (nullable = true)\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df4: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df4=spark.read.format(\"avro\").load(\"C:/Users/alvaro.romero/Big_Data/Ejercicios_Spark/Guardar_Tablas/AVRO/*\")\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bad6f355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parquetFile: String = C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet\r\n",
       "jsonFile: String = C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\r\n",
       "csvFile: String = C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\r\n",
       "orcFile: String = C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/orc/*\r\n",
       "avroFile: String = C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\r\n",
       "schema: String = DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, ...\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parquetFile=\"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet\"\n",
    "val jsonFile=\"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\"\n",
    "val csvFile=\"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\"\n",
    "val orcFile=\"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/orc/*\"\n",
    "val avroFile=\"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\"\n",
    "val schema= \"DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count INT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5ab8ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark\n",
    ".read\n",
    ".format(\"parquet\")\n",
    ".option(\"path\", parquetFile)\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad63177f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = spark.read.parquet(parquetFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83a9cf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d9b4825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    "            USING parquet\n",
    "            OPTIONS ( path \"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet\")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "820f4eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |1    |\n",
      "|United States    |Ireland            |264  |\n",
      "|United States    |India              |69   |\n",
      "|Egypt            |United States      |24   |\n",
      "|Equatorial Guinea|United States      |1    |\n",
      "|United States    |Singapore          |25   |\n",
      "|United States    |Grenada            |54   |\n",
      "|Costa Rica       |United States      |477  |\n",
      "|Senegal          |United States      |29   |\n",
      "|United States    |Marshall Islands   |44   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "734525aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark\n",
    ".read\n",
    ".format(\"json\")\n",
    ".option(\"path\", jsonFile)\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ecf5a6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "|United States    |India              |62   |\n",
      "|United States    |Singapore          |1    |\n",
      "|United States    |Grenada            |62   |\n",
      "|Costa Rica       |United States      |588  |\n",
      "|Senegal          |United States      |40   |\n",
      "|Moldova          |United States      |1    |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "df.show(10,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "645c6c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res20: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    "            USING json\n",
    "            OPTIONS ( path \"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ebd859d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "|United States    |India              |62   |\n",
      "|United States    |Singapore          |1    |\n",
      "|United States    |Grenada            |62   |\n",
      "|Costa Rica       |United States      |588  |\n",
      "|Senegal          |United States      |40   |\n",
      "|Moldova          |United States      |1    |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "489af577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark\n",
    ".read\n",
    ".format(\"csv\")\n",
    ".option(\"header\",\"true\")\n",
    ".schema(schema)\n",
    ".option(\"mode\",\"FAILFAST\")\n",
    ".option(\"nullValue\",\"\")\n",
    ".option(\"path\",csvFile)\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69a0c512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |1    |\n",
      "|United States    |Ireland            |264  |\n",
      "|United States    |India              |69   |\n",
      "|Egypt            |United States      |24   |\n",
      "|Equatorial Guinea|United States      |1    |\n",
      "|United States    |Singapore          |25   |\n",
      "|United States    |Grenada            |54   |\n",
      "|Costa Rica       |United States      |477  |\n",
      "|Senegal          |United States      |29   |\n",
      "|United States    |Marshall Islands   |44   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "df.show(10,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c0e5ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = spark\n",
    "  .read\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"mode\", \"FAILFAST\") \n",
    "  .option(\"nullValue\", \"\")\n",
    "  .schema(schema)\n",
    "  .csv(csvFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1305c025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |1    |\n",
      "|United States    |Ireland            |264  |\n",
      "|United States    |India              |69   |\n",
      "|Egypt            |United States      |24   |\n",
      "|Equatorial Guinea|United States      |1    |\n",
      "|United States    |Singapore          |25   |\n",
      "|United States    |Grenada            |54   |\n",
      "|Costa Rica       |United States      |477  |\n",
      "|Senegal          |United States      |29   |\n",
      "|United States    |Marshall Islands   |44   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "df2.show(10,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3602e149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res24: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    "    USING csv\n",
    "    OPTIONS (\n",
    "      path \"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\",\n",
    "      header \"true\",\n",
    "      inferSchema \"true\",\n",
    "      mode \"FAILFAST\"\n",
    "    )\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d5ee7bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |1    |\n",
      "|United States    |Ireland            |264  |\n",
      "|United States    |India              |69   |\n",
      "|Egypt            |United States      |24   |\n",
      "|Equatorial Guinea|United States      |1    |\n",
      "|United States    |Singapore          |25   |\n",
      "|United States    |Grenada            |54   |\n",
      "|Costa Rica       |United States      |477  |\n",
      "|Senegal          |United States      |29   |\n",
      "|United States    |Marshall Islands   |44   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "94c46c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark\n",
    ".read\n",
    ".format(\"orc\")\n",
    ".option(\"path\", orcFile)\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "28ed6448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |1    |\n",
      "|United States    |Ireland            |264  |\n",
      "|United States    |India              |69   |\n",
      "|Egypt            |United States      |24   |\n",
      "|Equatorial Guinea|United States      |1    |\n",
      "|United States    |Singapore          |25   |\n",
      "|United States    |Grenada            |54   |\n",
      "|Costa Rica       |United States      |477  |\n",
      "|Senegal          |United States      |29   |\n",
      "|United States    |Marshall Islands   |44   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "df.show(10,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c95fea46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res28: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    "    USING orc\n",
    "    OPTIONS (\n",
    "      path \"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/orc/*\"\n",
    "    )\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e0b2c97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |1    |\n",
      "|United States    |Ireland            |264  |\n",
      "|United States    |India              |69   |\n",
      "|Egypt            |United States      |24   |\n",
      "|Equatorial Guinea|United States      |1    |\n",
      "|United States    |Singapore          |25   |\n",
      "|United States    |Grenada            |54   |\n",
      "|Costa Rica       |United States      |477  |\n",
      "|Senegal          |United States      |29   |\n",
      "|United States    |Marshall Islands   |44   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7bd69156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read\n",
    "  .format(\"avro\")\n",
    "  .option(\"path\", avroFile)\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fe9b4fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |1    |\n",
      "|United States    |Ireland            |264  |\n",
      "|United States    |India              |69   |\n",
      "|Egypt            |United States      |24   |\n",
      "|Equatorial Guinea|United States      |1    |\n",
      "|United States    |Singapore          |25   |\n",
      "|United States    |Grenada            |54   |\n",
      "|Costa Rica       |United States      |477  |\n",
      "|Senegal          |United States      |29   |\n",
      "|United States    |Marshall Islands   |44   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "df.show(10,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "61a6fe55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res33: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    "    USING avro\n",
    "    OPTIONS (\n",
    "      path \"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\"\n",
    "    )\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "60cd3161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |1    |\n",
      "|United States    |Ireland            |264  |\n",
      "|United States    |India              |69   |\n",
      "|Egypt            |United States      |24   |\n",
      "|Equatorial Guinea|United States      |1    |\n",
      "|United States    |Singapore          |25   |\n",
      "|United States    |Grenada            |54   |\n",
      "|Costa Rica       |United States      |477  |\n",
      "|Senegal          |United States      |29   |\n",
      "|United States    |Marshall Islands   |44   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e5548b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n",
      "+------+-----+---------+----+-----+\n",
      "|height|width|nChannels|mode|label|\n",
      "+------+-----+---------+----+-----+\n",
      "|288   |384  |3        |16  |0    |\n",
      "|288   |384  |3        |16  |1    |\n",
      "|288   |384  |3        |16  |0    |\n",
      "|288   |384  |3        |16  |0    |\n",
      "|288   |384  |3        |16  |0    |\n",
      "+------+-----+---------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.source.image\r\n",
       "imageDir: String = C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/cctvVideos/train_images/\r\n",
       "imagesDF: org.apache.spark.sql.DataFrame = [image: struct<origin: string, height: int ... 4 more fields>, label: int]\r\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.source.image\n",
    "\n",
    "val imageDir = \"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/cctvVideos/train_images/\"\n",
    "val imagesDF = spark.read.format(\"image\").load(imageDir)\n",
    "\n",
    "imagesDF.printSchema\n",
    "imagesDF.select(\"image.height\", \"image.width\", \"image.nChannels\", \"image.mode\", \"label\").show(5,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "076e1a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+-----+\n",
      "|                path|   modificationTime|length|             content|label|\n",
      "+--------------------+-------------------+------+--------------------+-----+\n",
      "|file:/C:/Users/al...|2021-04-15 02:34:17| 55037|[FF D8 FF E0 00 1...|    0|\n",
      "|file:/C:/Users/al...|2021-04-15 02:34:17| 54634|[FF D8 FF E0 00 1...|    1|\n",
      "|file:/C:/Users/al...|2021-04-15 02:34:17| 54624|[FF D8 FF E0 00 1...|    0|\n",
      "|file:/C:/Users/al...|2021-04-15 02:34:17| 54505|[FF D8 FF E0 00 1...|    0|\n",
      "|file:/C:/Users/al...|2021-04-15 02:34:17| 54475|[FF D8 FF E0 00 1...|    0|\n",
      "+--------------------+-------------------+------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "binaryFilesDF: org.apache.spark.sql.DataFrame = [path: string, modificationTime: timestamp ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val binaryFilesDF = spark.read.format(\"binaryFile\")\n",
    ".option(\"pathGlobFilter\", \"*.jpg\")\n",
    ".load(imageDir)\n",
    "\n",
    "binaryFilesDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d322b2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+\n",
      "|                path|   modificationTime|length|             content|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "|file:/C:/Users/al...|2021-04-15 02:34:17| 55037|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/al...|2021-04-15 02:34:17| 54634|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/al...|2021-04-15 02:34:17| 54624|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/al...|2021-04-15 02:34:17| 54505|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/al...|2021-04-15 02:34:17| 54475|[FF D8 FF E0 00 1...|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "binaryFilesDF: org.apache.spark.sql.DataFrame = [path: string, modificationTime: timestamp ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val binaryFilesDF = spark.read.format(\"binaryFile\")\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\")\n",
    "  .option(\"recursiveFileLookup\", \"true\")\n",
    "  .load(imageDir)\n",
    "binaryFilesDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6463b3df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
