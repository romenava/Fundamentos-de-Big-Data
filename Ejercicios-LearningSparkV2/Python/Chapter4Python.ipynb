{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "da2f5ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.appName(\"SparkSQLExampleApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd3505cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile=\"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57c735e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightSchema = StructType([StructField(\"date\", StringType(), True),\n",
    "                                   StructField(\"delay\", IntegerType(), True),\n",
    "                                   StructField(\"distance\", IntegerType(),True),\n",
    "                                   StructField(\"origin\", StringType(), True),\n",
    "                                   StructField(\"destination\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "871d8c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =(spark.read.format(\"csv\")\n",
    ".schema(flightSchema)\n",
    ".option(\"header\",\"true\")\n",
    ".load(csvFile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca09b954",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81670a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|date    |delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|6    |602     |ABE   |ATL        |\n",
      "|01020600|-8   |369     |ABE   |DTW        |\n",
      "|01021245|-2   |602     |ABE   |ATL        |\n",
      "|01020605|-4   |602     |ABE   |ATL        |\n",
      "|01031245|-4   |602     |ABE   |ATL        |\n",
      "|01030605|0    |602     |ABE   |ATL        |\n",
      "|01041243|10   |602     |ABE   |ATL        |\n",
      "|01040605|28   |602     |ABE   |ATL        |\n",
      "|01051245|88   |602     |ABE   |ATL        |\n",
      "|01050605|9    |602     |ABE   |ATL        |\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT *\n",
    "            FROM us_delay_flights_tbl\"\"\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec0397af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT distance,origin,destination \n",
    "            FROM us_delay_flights_tbl\n",
    "            WHERE distance >1000\n",
    "            ORDER BY distance DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41748255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df.select(\"distance\", \"origin\", \"destination\")\n",
    ".filter(col(\"distance\")>1000)\n",
    ".orderBy(desc(\"distance\"))\n",
    ".show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20915a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------+-----------+\n",
      "|delay|    date|origin|destination|\n",
      "+-----+--------+------+-----------+\n",
      "| 1638|02190925|   SFO|        ORD|\n",
      "|  396|01031755|   SFO|        ORD|\n",
      "|  326|01022330|   SFO|        ORD|\n",
      "|  320|01051205|   SFO|        ORD|\n",
      "|  297|01190925|   SFO|        ORD|\n",
      "|  296|02171115|   SFO|        ORD|\n",
      "|  279|01071040|   SFO|        ORD|\n",
      "|  274|01051550|   SFO|        ORD|\n",
      "|  266|03120730|   SFO|        ORD|\n",
      "|  258|01261104|   SFO|        ORD|\n",
      "+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT delay,date,origin,destination \n",
    "            FROM us_delay_flights_tbl\n",
    "            WHERE origin='SFO' and destination='ORD' and delay > 120 \n",
    "            ORDER BY delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c03b7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------+-----------+\n",
      "|delay|    date|origin|destination|\n",
      "+-----+--------+------+-----------+\n",
      "|  122|01011237|   SFO|        ORD|\n",
      "|  123|02131320|   SFO|        ORD|\n",
      "|  123|03311600|   SFO|        ORD|\n",
      "|  124|01011410|   SFO|        ORD|\n",
      "|  125|03051115|   SFO|        ORD|\n",
      "|  126|01041205|   SFO|        ORD|\n",
      "|  131|01031550|   SFO|        ORD|\n",
      "|  134|02021115|   SFO|        ORD|\n",
      "|  137|02081104|   SFO|        ORD|\n",
      "|  139|03311810|   SFO|        ORD|\n",
      "+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df\n",
    ".select(\"delay\", \"date\", \"origin\", \"destination\")\n",
    ".where((col(\"origin\")==\"SFO\") & (col(\"destination\")==\"ORD\") & (col(\"delay\")>120))\n",
    ".orderBy(\"delay\")\n",
    ".show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1710424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "new_df=df.withColumn(\"DateTime\", to_timestamp(col(\"date\"), \"MMddHHmm\")).drop(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "082a7b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67700ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n",
      "|Day|Month|Count|\n",
      "+---+-----+-----+\n",
      "| 31|    3|    4|\n",
      "|  2|    1|    4|\n",
      "|  3|    1|    4|\n",
      "|  9|    2|    3|\n",
      "| 12|    3|    3|\n",
      "| 26|    3|    2|\n",
      "| 17|    3|    2|\n",
      "| 27|    2|    2|\n",
      "|  1|    1|    2|\n",
      "|  5|    1|    2|\n",
      "+---+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT day(DateTime) Day,month(DateTime) Month, count(*) Count\n",
    "            FROM us_delay_flights_tbl\n",
    "            WHERE origin='SFO' and destination='ORD' and delay > 120 \n",
    "            Group by month(DateTime), day(DateTime)\n",
    "            ORDER BY count(*) DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e2437163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+-----+\n",
      "|month(DateTime)|dayofmonth(DateTime)|count|\n",
      "+---------------+--------------------+-----+\n",
      "|              1|                   3|    4|\n",
      "|              1|                   2|    4|\n",
      "|              3|                  31|    4|\n",
      "|              3|                  12|    3|\n",
      "|              2|                   9|    3|\n",
      "|              3|                  26|    2|\n",
      "|              3|                  17|    2|\n",
      "|              1|                   5|    2|\n",
      "|              1|                  30|    2|\n",
      "|              1|                   1|    2|\n",
      "+---------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(new_df\n",
    ".where((col(\"origin\")==\"SFO\") & (col(\"destination\")==\"ORD\") & (col(\"delay\")>120))\n",
    ".groupBy(month(col(\"DateTime\")), dayofmonth(col(\"DateTime\")))\n",
    ".count()\n",
    ".orderBy(desc(\"count\")).show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3f23c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    "             CASE\n",
    "                 WHEN delay >= 360 THEN 'Very Long Delays'\n",
    "                 WHEN delay >= 120 AND delay < 360 THEN 'Long Delays'\n",
    "                 WHEN delay >= 60 AND delay < 120 THEN 'Short Delays'\n",
    "                 WHEN delay >= 0 AND delay < 60 THEN 'Tolerable Delays'\n",
    "                 WHEN delay = 0 THEN 'No Delays'\n",
    "                 ELSE 'Early'\n",
    "             END AS Flight_Delays\n",
    "             FROM us_delay_flights_tbl\n",
    "             ORDER BY origin, delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a11112d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df\n",
    ".select(col(\"delay\"), col(\"origin\"), col(\"destination\"),\n",
    "                 when(col(\"delay\") >= 360,\"Very Long Delays\")\n",
    "                 .when((col(\"delay\") >= 120) & (col(\"delay\") < 360),\"Long Delays\")\n",
    "                 .when((col(\"delay\") >= 60) & (col(\"delay\") < 120),\"Short Delays\")\n",
    "                 .when((col(\"delay\")>= 0) & (col(\"delay\") < 60),\"Tolerable Delays\")\n",
    "                 .when(col(\"delay\") == 0, \"No Delays\")\n",
    "                 .otherwise(\"Early\")\n",
    "                 .alias(\"Flight_Delays\"))\n",
    ".orderBy(asc(\"origin\"),desc(\"delay\"))\n",
    ".show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f629e0f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd3ee046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT, distance INT, origin STRING, destination STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93ba0b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE us_delay_flights_tbl(date STRING, delay INT, distance INT, origin STRING, destination STRING)\n",
    "             USING csv OPTIONS (PATH 'C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/departuredelays.csv')\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02c81a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='Default Hive database', locationUri='file:/C:/Users/alvaro.romero/Big_Data/spark-warehouse'),\n",
       " Database(name='learn_spark_db', description='', locationUri='file:/C:/Users/alvaro.romero/Big_Data/spark-warehouse/learn_spark_db.db')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63b76190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='managed_us_delay_flights_tbl', database='learn_spark_db', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='us_delay_flights_tbl', database='learn_spark_db', description=None, tableType='EXTERNAL', isTemporary=False)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11ba27dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='date', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='delay', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='distance', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='origin', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='destination', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f09a115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallNumber: integer (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: integer (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: integer (nullable = true)\n",
      " |-- ALSUnit: boolean (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumAlarms: integer (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: integer (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- ResponseDelayedMins: float (nullable = true)\n",
      " |-- IncidentDate: timestamp (nullable = true)\n",
      " |-- OnWatchDate: timestamp (nullable = true)\n",
      " |-- AvailableDtTS: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.format(\"parquet\").load(\"C:/Users/alvaro.romero/Big_Data/Ejercicios_Spark/Guardar_Tablas2/Parquet/*\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "108148fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ALSUnit: boolean (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- AvailableDtTS: string (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- CallNumber: long (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- FinalPriority: long (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- IncidentDate: string (nullable = true)\n",
      " |-- IncidentNumber: long (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- NumAlarms: long (nullable = true)\n",
      " |-- OnWatchDate: string (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- ResponseDelayedMins: double (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- SupervisorDistrict: string (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: long (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- Zipcode: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=spark.read.format(\"json\").load(\"C:/Users/alvaro.romero/Big_Data/Ejercicios_Spark/Guardar_Tablas2/JSON/*\")\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "febbd2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: integer (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: integer (nullable = true)\n",
      " |-- _c14: boolean (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: integer (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: integer (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      " |-- _c20: string (nullable = true)\n",
      " |-- _c21: string (nullable = true)\n",
      " |-- _c22: string (nullable = true)\n",
      " |-- _c23: string (nullable = true)\n",
      " |-- _c24: double (nullable = true)\n",
      " |-- _c25: timestamp (nullable = true)\n",
      " |-- _c26: timestamp (nullable = true)\n",
      " |-- _c27: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3=(spark.read.format(\"csv\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".option(\"mode\",\"PERMISSIVE\").load(\"C:/Users/alvaro.romero/Big_Data/Ejercicios_Spark/Guardar_Tablas2/CSV/*\"))\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae13724b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallNumber: integer (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: integer (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: integer (nullable = true)\n",
      " |-- ALSUnit: boolean (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumAlarms: integer (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: integer (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- ResponseDelayedMins: float (nullable = true)\n",
      " |-- IncidentDate: timestamp (nullable = true)\n",
      " |-- OnWatchDate: timestamp (nullable = true)\n",
      " |-- AvailableDtTS: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4=spark.read.format(\"avro\").load(\"C:/Users/alvaro.romero/Big_Data/Ejercicios_Spark/Guardar_Tablas2/AVRO/*\")\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b649ac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetFile=\"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet\"\n",
    "jsonFile=\"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\"\n",
    "csvFile=\"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\"\n",
    "orcFile=\"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/orc/*\"\n",
    "avroFile=\"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\"\n",
    "schema= \"DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count INT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eaacd6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark\n",
    ".read\n",
    ".format(\"parquet\")\n",
    ".option(\"path\", parquetFile)\n",
    ".load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "92c54b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.parquet(parquetFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "de6610d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "|    United States|          Singapore|   25|\n",
      "|    United States|            Grenada|   54|\n",
      "|       Costa Rica|      United States|  477|\n",
      "|          Senegal|      United States|   29|\n",
      "|    United States|   Marshall Islands|   44|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "|    United States|          Singapore|   25|\n",
      "|    United States|            Grenada|   54|\n",
      "|       Costa Rica|      United States|  477|\n",
      "|          Senegal|      United States|   29|\n",
      "|    United States|   Marshall Islands|   44|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)\n",
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "70215490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    "            USING parquet\n",
    "            OPTIONS ( path \"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet\")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c0888711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |1    |\n",
      "|United States    |Ireland            |264  |\n",
      "|United States    |India              |69   |\n",
      "|Egypt            |United States      |24   |\n",
      "|Equatorial Guinea|United States      |1    |\n",
      "|United States    |Singapore          |25   |\n",
      "|United States    |Grenada            |54   |\n",
      "|Costa Rica       |United States      |477  |\n",
      "|Senegal          |United States      |29   |\n",
      "|United States    |Marshall Islands   |44   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bbd558e2",
   "metadata": {},
   "outputs": [],
   "source": [
    " df = (spark\n",
    ".read\n",
    ".format(\"json\")\n",
    ".option(\"path\", jsonFile)\n",
    ".load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eeea2c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "|United States    |India              |62   |\n",
      "|United States    |Singapore          |1    |\n",
      "|United States    |Grenada            |62   |\n",
      "|Costa Rica       |United States      |588  |\n",
      "|Senegal          |United States      |40   |\n",
      "|Moldova          |United States      |1    |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ddd24afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    "            USING json\n",
    "            OPTIONS ( path \"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "29ef2a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "|United States    |India              |62   |\n",
      "|United States    |Singapore          |1    |\n",
      "|United States    |Grenada            |62   |\n",
      "|Costa Rica       |United States      |588  |\n",
      "|Senegal          |United States      |40   |\n",
      "|Moldova          |United States      |1    |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "119b7f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark\n",
    ".read\n",
    ".format(\"csv\")\n",
    ".option(\"header\",\"true\")\n",
    ".schema(schema)\n",
    ".option(\"mode\",\"FAILFAST\")\n",
    ".option(\"nullValue\",\"\")\n",
    ".option(\"path\",csvFile)\n",
    ".load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "992852d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |1    |\n",
      "|United States    |Ireland            |264  |\n",
      "|United States    |India              |69   |\n",
      "|Egypt            |United States      |24   |\n",
      "|Equatorial Guinea|United States      |1    |\n",
      "|United States    |Singapore          |25   |\n",
      "|United States    |Grenada            |54   |\n",
      "|Costa Rica       |United States      |477  |\n",
      "|Senegal          |United States      |29   |\n",
      "|United States    |Marshall Islands   |44   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "609bad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = (spark\n",
    "  .read\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"mode\", \"FAILFAST\") \n",
    "  .option(\"nullValue\", \"\")\n",
    "  .schema(schema)\n",
    "  .csv(csvFile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2bcb13c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |1    |\n",
      "|United States    |Ireland            |264  |\n",
      "|United States    |India              |69   |\n",
      "|Egypt            |United States      |24   |\n",
      "|Equatorial Guinea|United States      |1    |\n",
      "|United States    |Singapore          |25   |\n",
      "|United States    |Grenada            |54   |\n",
      "|Costa Rica       |United States      |477  |\n",
      "|Senegal          |United States      |29   |\n",
      "|United States    |Marshall Islands   |44   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "457305be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    "    USING csv\n",
    "    OPTIONS (\n",
    "      path \"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\",\n",
    "      header \"true\",\n",
    "      inferSchema \"true\",\n",
    "      mode \"FAILFAST\"\n",
    "    )\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "453740fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |1    |\n",
      "|United States    |Ireland            |264  |\n",
      "|United States    |India              |69   |\n",
      "|Egypt            |United States      |24   |\n",
      "|Equatorial Guinea|United States      |1    |\n",
      "|United States    |Singapore          |25   |\n",
      "|United States    |Grenada            |54   |\n",
      "|Costa Rica       |United States      |477  |\n",
      "|Senegal          |United States      |29   |\n",
      "|United States    |Marshall Islands   |44   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ac5b94a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark\n",
    ".read\n",
    ".format(\"orc\")\n",
    ".option(\"path\", orcFile)\n",
    ".load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "be55536d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |1    |\n",
      "|United States    |Ireland            |264  |\n",
      "|United States    |India              |69   |\n",
      "|Egypt            |United States      |24   |\n",
      "|Equatorial Guinea|United States      |1    |\n",
      "|United States    |Singapore          |25   |\n",
      "|United States    |Grenada            |54   |\n",
      "|Costa Rica       |United States      |477  |\n",
      "|Senegal          |United States      |29   |\n",
      "|United States    |Marshall Islands   |44   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8ebc428c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    "    USING orc\n",
    "    OPTIONS (\n",
    "      path \"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/orc/*\"\n",
    "    )\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "121b9015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |1    |\n",
      "|United States    |Ireland            |264  |\n",
      "|United States    |India              |69   |\n",
      "|Egypt            |United States      |24   |\n",
      "|Equatorial Guinea|United States      |1    |\n",
      "|United States    |Singapore          |25   |\n",
      "|United States    |Grenada            |54   |\n",
      "|Costa Rica       |United States      |477  |\n",
      "|Senegal          |United States      |29   |\n",
      "|United States    |Marshall Islands   |44   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5e5a86df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "  .format(\"avro\")\n",
    "  .option(\"path\", avroFile)\n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "454c8afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |1    |\n",
      "|United States    |Ireland            |264  |\n",
      "|United States    |India              |69   |\n",
      "|Egypt            |United States      |24   |\n",
      "|Equatorial Guinea|United States      |1    |\n",
      "|United States    |Singapore          |25   |\n",
      "|United States    |Grenada            |54   |\n",
      "|Costa Rica       |United States      |477  |\n",
      "|Senegal          |United States      |29   |\n",
      "|United States    |Marshall Islands   |44   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "014be2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    "    USING avro\n",
    "    OPTIONS (\n",
    "      path \"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\"\n",
    "    )\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "88ddeb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |1    |\n",
      "|United States    |Ireland            |264  |\n",
      "|United States    |India              |69   |\n",
      "|Egypt            |United States      |24   |\n",
      "|Equatorial Guinea|United States      |1    |\n",
      "|United States    |Singapore          |25   |\n",
      "|United States    |Grenada            |54   |\n",
      "|Costa Rica       |United States      |477  |\n",
      "|Senegal          |United States      |29   |\n",
      "|United States    |Marshall Islands   |44   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a6e755e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+----+-----+\n",
      "|height|width|nChannels|mode|label|\n",
      "+------+-----+---------+----+-----+\n",
      "|288   |384  |3        |16  |0    |\n",
      "|288   |384  |3        |16  |1    |\n",
      "|288   |384  |3        |16  |0    |\n",
      "|288   |384  |3        |16  |0    |\n",
      "|288   |384  |3        |16  |0    |\n",
      "+------+-----+---------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import image\n",
    "\n",
    "imageDir = \"C:/Users/alvaro.romero/Big_Data/LearningSparkV2-master/databricks-datasets/learning-spark-v2/cctvVideos/train_images/\"\n",
    "imagesDF = spark.read.format(\"image\").load(imageDir)\n",
    "\n",
    "imagesDF.printSchema\n",
    "imagesDF.select(\"image.height\", \"image.width\", \"image.nChannels\", \"image.mode\", \"label\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6c6d47ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+-----+\n",
      "|                path|   modificationTime|length|             content|label|\n",
      "+--------------------+-------------------+------+--------------------+-----+\n",
      "|file:/C:/Users/al...|2021-04-15 02:34:17| 55037|[FF D8 FF E0 00 1...|    0|\n",
      "|file:/C:/Users/al...|2021-04-15 02:34:17| 54634|[FF D8 FF E0 00 1...|    1|\n",
      "|file:/C:/Users/al...|2021-04-15 02:34:17| 54624|[FF D8 FF E0 00 1...|    0|\n",
      "|file:/C:/Users/al...|2021-04-15 02:34:17| 54505|[FF D8 FF E0 00 1...|    0|\n",
      "|file:/C:/Users/al...|2021-04-15 02:34:17| 54475|[FF D8 FF E0 00 1...|    0|\n",
      "+--------------------+-------------------+------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binaryFilesDF = (spark.read.format(\"binaryFile\")\n",
    ".option(\"pathGlobFilter\", \"*.jpg\")\n",
    ".load(imageDir))\n",
    "\n",
    "binaryFilesDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a20c8e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+\n",
      "|                path|   modificationTime|length|             content|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "|file:/C:/Users/al...|2021-04-15 02:34:17| 55037|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/al...|2021-04-15 02:34:17| 54634|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/al...|2021-04-15 02:34:17| 54624|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/al...|2021-04-15 02:34:17| 54505|[FF D8 FF E0 00 1...|\n",
      "|file:/C:/Users/al...|2021-04-15 02:34:17| 54475|[FF D8 FF E0 00 1...|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binaryFilesDF = (spark.read.format(\"binaryFile\")\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\")\n",
    "  .option(\"recursiveFileLookup\", \"true\")\n",
    "  .load(imageDir))\n",
    "binaryFilesDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d69bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
